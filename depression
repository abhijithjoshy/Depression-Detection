

print("\n\n\n")
import itertools
import threading
import time
import sys
for i in range(25):
    sys.stdout.write('\rfinding load time ' + "..")
    time.sleep(0.010)
    sys.stdout.write('\rfinding load time ' + "....")
    sys.stdout.flush()
    time.sleep(0.010)
    sys.stdout.write('\rfinding load time ' + "........")
    time.sleep(0.010)
    sys.stdout.write('\rfinding load time ' + "................")
    sys.stdout.flush()
    time.sleep(0.010)
    sys.stdout.write('\rfinding load time ' + "......................")
    time.sleep(0.010)
    sys.stdout.write('\rfinding load time' + ".....................................")
    sys.stdout.flush()
    time.sleep(0.010)
    sys.stdout.write('\r       Just 15 seconds!  Please wait !                                                                                                                 ')
print("\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n")



import matplotlib.pyplot as plt
import nltk
import numpy as np
import re 
import pandas as pd
import string
import seaborn as sns

from nltk.corpus import stopwords  
from nltk.stem.lancaster import LancasterStemmer 

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer




import nltk
nltk.download('stopwords')






set(stopwords.words('english'))





data = pd.read_csv("C:\\Users\\abhij\\Downloads\\normal8000.csv")
data.head()












data.Normal.value_counts(normalize=True)





data.head()






data_count=data.iloc[:,2:].sum()



num_rows = len(data)
print(num_rows)









import re
import string

alphanumeric = lambda x: re.sub('\w*\d\w*', ' ', x)


punc_lower = lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x.lower())


remove_n = lambda x: re.sub("\n", " ", x)

remove_non_ascii = lambda x: re.sub(r'[^\x00-\x7f]',r' ', x)


data['comment_text'] = data['comment_text'].map(alphanumeric).map(punc_lower).map(remove_n).map(remove_non_ascii)

data['comment_text'][0]
data['comment_text'][3]




data_normal = data.loc[:,['id','comment_text','Normal']]





data_normal['comment_text'][0]



data_normal['comment_text'][1]




data_miled = data.loc[:,['id','comment_text','miled']]
data_miled.tail(20)




data_borderline = data.loc[:,['id','comment_text','miled']]





data_moderate = data.loc[:,['id','comment_text','borderline']]





data_severe = data.loc[:,['id','comment_text','moderate']]




data_extreme = data.loc[:,['id','comment_text','extreme']]




data_normal.head()




data_normal_1 = data_normal[data_normal['Normal'] == 1].iloc[0:5000,:]
data_normal_1.shape




data_normal_0 = data_normal[data_normal['Normal'] == 0].iloc[0:5000,:]




data_normal_done = pd.concat([data_normal_1, data_normal_0], axis=0)
data_normal_done.shape




data_miled[data_miled['miled'] == 1].count()




data_miled_1 = data_miled[data_miled['miled'] == 1].iloc[0:1595,:]
data_miled_0 = data_miled[data_miled['miled'] == 0].iloc[0:1595,:]
data_miled_done = pd.concat([data_miled_1, data_miled_0], axis=0)
data_miled_done.shape





data_borderline[data_borderline['miled'] == 1].count()




data_borderline_1 = data_borderline[data_borderline['miled'] == 1].iloc[0:5000,:]
data_borderline_0 = data_borderline[data_borderline['miled'] == 0].iloc[0:5000,:]
data_borderline_done = pd.concat([data_borderline_1, data_borderline_0], axis=0)
data_borderline_done.shape


data_moderate[data_moderate['borderline'] == 1].count()




data_moderate_1 = data_moderate[data_moderate['borderline'] == 1].iloc[0:478,:]


data_moderate_0 = data_moderate[data_moderate['borderline'] == 0].iloc[0:1912,:]  
data_moderate_done = pd.concat([data_moderate_1, data_moderate_0], axis=0)
data_moderate_done.shape





data_severe[data_severe['moderate'] == 1].count()




data_severe_1 = data_severe[data_severe['moderate'] == 1].iloc[0:5000,:]
data_severe_0 = data_severe[data_severe['moderate'] == 0].iloc[0:5000,:]
data_severe_done = pd.concat([data_severe_1, data_severe_0], axis=0)
data_severe_done.shape




data_extreme[data_extreme['extreme'] == 1].count()




data_extreme_1 = data_extreme[data_extreme['extreme'] == 1].iloc[0:1405,:] # 20%
data_extreme_0 = data_extreme[data_extreme['extreme'] == 0].iloc[0:5620,:] # 80%
data_extreme_done = pd.concat([data_extreme_1, data_extreme_0], axis=0)
data_extreme_done.shape
data_extreme_done




from tkinter import *
root = Tk()
root.title("Depression Detection System")
root.configure(bg='#FFDBE1')

root.geometry("2000x2000")

l2=Label(background='#FFDBE1',text="You can type a sentence below to detect the depression level",font=("",15,"bold"))
l2.place(x=370,y=100)






from sklearn import preprocessing
from sklearn.feature_selection import SelectFromModel

from sklearn.model_selection import train_test_split, KFold, cross_val_score
from sklearn.metrics import f1_score, precision_score, recall_score, precision_recall_curve, fbeta_score, confusion_matrix
from sklearn.metrics import roc_auc_score, roc_curve

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import MultinomialNB, BernoulliNB
from sklearn.svm import LinearSVC
from sklearn.ensemble import RandomForestClassifier




'''
df_done: data_normal_done, data_miled_done, ...
label: Normal, miled, ...
vectorizer values: CountVectorizer, TfidfVectorizer
gram_range values: (1,1) for unigram, (2,2) for bigram
'''
def cv_tf_train_test(df_done,label,vectorizer,ngram):

    ''' Train/Test split'''
    X = df_done.comment_text
    y = df_done[label]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

    ''' Count Vectorizer/TF-IDF '''

    cv1 = vectorizer(ngram_range=(ngram), stop_words='english')
    
    X_train_cv1 = cv1.fit_transform(X_train) 
    X_test_cv1  = cv1.transform(X_test)      
    
  
        
    ''' Initialize all model objects and fit the models on the training data '''
    lr = LogisticRegression()
    lr.fit(X_train_cv1, y_train)
    print('lr done')

    knn = KNeighborsClassifier(n_neighbors=5)
    knn.fit(X_train_cv1, y_train)

    bnb = BernoulliNB()
    bnb.fit(X_train_cv1, y_train)
    print('bnb done')
    
    mnb = MultinomialNB()
    mnb.fit(X_train_cv1, y_train)
    print('mnb done')
    
    svm_model = LinearSVC()
    svm_model.fit(X_train_cv1, y_train)

    randomforest = RandomForestClassifier(n_estimators=100, random_state=42)
    randomforest.fit(X_train_cv1, y_train)
    print('rdf done')
    
   
    f1_score_data = {'F1 Score':[f1_score(lr.predict(X_test_cv1), y_test), f1_score(knn.predict(X_test_cv1), y_test), 
                                f1_score(bnb.predict(X_test_cv1), y_test), f1_score(mnb.predict(X_test_cv1), y_test),
                                f1_score(svm_model.predict(X_test_cv1), y_test), f1_score(randomforest.predict(X_test_cv1), y_test)]} 
                          
   
    df_f1 = pd.DataFrame(f1_score_data, index=['Log Regression','KNN', 'BernoulliNB', 'MultinomialNB', 'SVM', 'Random Forest'])  

    return df_f1





'''
def cv_tf_train_test(df_done,label,vectorizer,ngram)
vectorizer values: CountVectorizer, TfidfVectorizer
ngram_range values: (1,1) for unigram, (2,2) for bigram
'''



X = data_normal_done.comment_text
y = data_normal_done['Normal']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

tfv = TfidfVectorizer(ngram_range=(1,1), stop_words='english')

X_train_fit = tfv.fit_transform(X_train)  
X_test_fit = tfv.transform(X_test) 
randomforest = RandomForestClassifier(n_estimators=100, random_state=42)

randomforest.fit(X_train_fit, y_train)
randomforest.predict(X_test_fit)









og1=pd.read_csv("D:\MSC Computer Science\Project\datasetog.csv")
og=og1["Tweet"]



import re
import string

alphanumeric = lambda x: re.sub('\w*\d\w*', ' ', x)


punc_lower = lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x.lower())

remove_n = lambda x: re.sub("\n", " ", x)

remove_non_ascii = lambda x: re.sub(r'[^\x00-\x7f]',r' ', x)

og = og.map(alphanumeric).map(punc_lower).map(remove_n).map(remove_non_ascii)







ee=[]
ff=[]
i=0
for KK in og:
    a=og[i]
    comment1 = [a]
    dd=og1["Name"]
    nk=dd[i]
    comment1_vect = tfv.transform(comment1)
    m=randomforest.predict_proba(comment1_vect)[:,1]
    if m < 0.2:
        
        e = "Considered normal"
        f=nk
    elif m < 0.4 and m >0.2:
       
        e="Miled depression"
        f=nk
    elif m < 0.6 and m >0.4:
        
        e="Borderline depression"
        f=nk
    elif m < 0.8 and m > 0.6:
      
        e="Severe depression      "
        f=nk 
    else :
       
        e="Extreme depression"
        f=nk
    i = i+1
    ee.append(e)
    ff.append(f)


def a():
   
  a=inputtxt.get("1.0", "end-1c")
  comment1 = [a]
  comment1_vect = tfv.transform(comment1)
  m=randomforest.predict_proba(comment1_vect)[:,1]
  if m < 0.2:
    dep = print("Considered normal")
    dep = "Considered normal( 0-20 % )    "
    l1=Label(text=dep,fg="green",font=("",15,"bold"))
    l1.place(x=500,y=350)
  elif m < 0.4 and m >0.2:
    dep = print("miled depression      ")
    dep = "miled depression( 20-40%)   "
    l1=Label(text=dep,fg="blue",font=("",15,"bold"))
    l1.place(x=500,y=350)
  elif m < 0.6 and m >0.4:
    dep = print("Borderline depression")
    dep = "Borderline depression( 40-60%)"
    l1=Label(text=dep,fg="orange",font=("",15,"bold"))
    l1.place(x=500,y=350)
  elif m < 0.8 and m > 0.6:
     dep = print("Severe depression     ")
     dep = "Severe depression( 60-80%)     "   
     l1=Label(text=dep,fg="brown",font=("",15,"bold"))
     l1.place(x=500,y=350)
  else :
    dep = print("Extreme depression")
    dep ="Extreme depression( 80-100%)  "
    l1=Label(text=dep,fg="red",font=("",15,"bold"))
    l1.place(x=500,y=350)





def todataset():
    import tkinter as tk

    top = Tk()  
    top.wm_attributes('-toolwindow', 'True')
    top.title("Depression detection system")
    top.geometry("10000x10000")
    top.configure(bg='#FFDBE1')
    def datadisp():
        #sb = Scrollbar(top)  
        #sb.pack(side = RIGHT, fill = Y)  
        #sb.grid()  
        #mylist = Listbox(top, yscrollcommand = sb.set )  
        mylist = Listbox(top)  
        kk=0
        for line in ee:  
            h=ff[kk]+"....................................."+ee[kk]
            mylist.insert(END, h)  
            kk=kk+1
        #mylist.pack( side = TOP,fill = BOTH, expand = True )  
        mylist.place(x=100,y=150,height=500,width=400)
        #bolded = font.Font(family='Helvetica', size=12) # will use the default font
        #mylist.config(font=bolded)
        labeldisp=Label(top,text="Name       \t          Depression level ",font=("",15,""))    
        labeldisp.place(x=100,y=100)
    dispbutton=Button(top,text="Display",command=datadisp)
    dispbutton.place(x=10,y=50)
    top.mainloop()  
   

inputtxt = Text(height = 5,width = 20)
inputtxt.place(x=560,y=180)
b1=Button(text="Enter",command=a)
b1.place(x=600,y=290,width=80)
linsert=Label(background='#FFDBE1',text="Click below to insert the posts of a large amount of people and find corresponding depression level!",font=("",15,""))
linsert.place(x=210,y=450)
bdataprint=Button(text="Insert",command=todataset)
bdataprint.place(x=600,y=550,width=80) 




def contactl():
    contact1=Tk()
    contact1.wm_attributes('-toolwindow', 'True')
    contact1.geometry("500x500")
    lcontact=Label(contact1,text="abhijithjoshy500@gmail.com",font=("",10,""))
    lcontact.place(x=10,y=100)
    contact1.mainloop()
menubar=Menu(root)
def destroy1():
    root.destroy()
    contact1.destroy()
    donothing.destroy()
def donothing():
   about1=Tk()
   about1.wm_attributes('-toolwindow', 'True')
   about1.geometry("500x1000")
   labout = Label(about1,font=("",12,""), text="""This system can be used to find the depression level of individuals
   Depression is a serious mental health issue for 
   people world-wide irrelevant of their ages, genders and races.   
   In this age of modern communication and technology, people 
   feel more comfortable sharing their thoughts in social 
    networking sites (SNS) almost every day.Undiagnosed and 
    untreated disorders created from 
    depression can lead a patient to chronic illnesses even to 
    suicide. Over 300 million people all over the world are 
    estimated to suffer from depression which is equivalent to 
    4.4% of the worldâ€™s population. Sometimes depression at 
    its worst, can lead to suicide. Around 800000 people die 
    committing suicide every year. Suicide is the second leading 
    cause of death in for 15-29 years old. The symptoms of 
    depression can be categorized in three ways: psychological, 
    social and physical. Usually a patient is unlikely to have 
    all of these symptoms, but these symptoms can predict the 
    severity of depression. Continuous sadness, hopelessness, 
    low self-esteem, anxiousness, feeling guilty can be treated as 
    the psychological symptoms. Social symptoms include avoiding 
    friends and family, indifference towards almost 
    every activity etc. Depression is also responsible for many 
    other physical illnesses as well. People with depression may 
    experience appetite changes, which can cause unintended 
    weight loss or gain. Also they may experience unexplained 
    aches or pains, including joint or muscle pain, breast 
    tenderness, and headaches. Depression has a high 
    treatment rate but nearly two out of three people suffering 
    with depression do not actively seek nor receive proper 
    treatment.""")
   labout.place(x=10,y=10)
   about1.mainloop()
filemenu = Menu(menubar, tearoff=0)
filemenu.add_command(label="About", command=donothing)
filemenu.add_command(label="Contact us", command=contactl)
filemenu.add_separator()

filemenu.add_command(label="Exit", command=destroy1)
menubar.add_cascade(label="Options", menu=filemenu)
editmenu = Menu(menubar, tearoff=0)
editmenu.add_command(label="Undo", command=donothing)

root.config(menu=menubar)
root.wm_attributes('-toolwindow', 'True')


root.mainloop()    

